```{r load_requirements, eval = T, include = F}  
# this loads the befdata package
require("rbefdata")  
require("ggplot2") 
```

## Abstract 

We face a deluge of data scientists need to deal with in many different
disciplines today. While there are already good solutions to some parts of the
data life cycle the applicability of the solutions to certain scientific
domains often varies. Especially research domains with high degree of
interdisciplinary interactions and heterogeneity in methods and data in general
like ecology face problems in dealing with some valuable concepts like
ontologies that potentially can be used to improve or automate some of the most
common tasks in analyses like finding relevant data, cleaning and merging of
datasets. We here introduce the `rbefata` package that connects to the open
source data management platform `BEFdata` that has been developed and is used
within the BEF-China experiment. We show the use of the package in combination
with the portal using an example workflow that integrates three datasets from
the BEF-China experiment representing an analysis that has been published
already.  We discuss the combination of the R package `rbefdata` and the data
portal in the context of state of the art data management as well as we give an
outlook on upcoming features that will bring semantical features like smart
merges based on an ontology we created. 

## Introduction 

With a growing awareness on value of data, much effort has been put into
building data management platforms, to preserve all kind of environmental and
historic data, over the last years (e.g. diversity workbench, BEFdata). Many
specialized solutions for different scientific disciplines appeared that
provide data management plans for small scale projects or collaborations as
well as for large data producing long term or remote sensing projects. An
ongoing trend in that context is the development of integrative databases or
data portals. They serve as nodes that collect data from smaller databases of a
certain domain and they give researchers of that domain the opportunity to
access a wide range of relevant data all from one place. These data management
portals in fact offer a solution to to one of the most pressing problems that
we face with our valuable data today, their lost. 

Another big problem, especially in terms of reuse of available data, is the
general understanding of datasets. Usually plain datasets say nothing, to one
who is not familiar with it and they are even hard to decipher by the author
itself after some time has passed. It is usually hard to remember exactly what
methods have been used to collect a certain columns data or what the
abbreviations or headers in the dataset mean. To solve this problem metadata
frameworks have been developed and published as standards so nobody really
needs to think about an own set of requirements to describe its data. The
Ecological Metadata Language is only one example for that. While this
theoretically solves the problem with not well described datasets it is still
hard to make people use it extensively as this usually always means to learn
new tools that help with the description process (e.g morpho, data up).

While well described data can help a lot in understanding datasets and on
deciding upon the relevance and applicability in a certain analysis there is
still lots of manual intervention necessary after that to prepare the data for
analysis (cite yourself? or xxx). It may needs to be cleaned, imputed, reshaped
and merged which usually takes up to 70% of an analysis workflow, before the
smart models can be applied to the data to find interesting patters (cite the
workflow paper of Karin and me). This preparation steps not only are time and
labour intensive but also potentially error prone, especially as the complexity
of analyses grows. 

Ontologies, formal representations of knowledge, potentially offer a
sophisticated tool to deal with that step of data preparation (cite supporting
ecology as data intensive science). While they are already used in some
research domains like genetics (cite xxx), other domains face more problems
using it. For example in ecology, that has grown into a very collaborative,
interdisciplinary and data intensive science over the last decade, to address
questions on a greater temporal and spatial scale (e.g michener et al 2012).
The data here is mainly provided by small scale studies spread all over the
world (e.g heidorn2009 shedding light on the dark) but also through bigger long
term projects like LTER (cite xxx), BEF-China (cite xxx), governmental projects
and local initiatives (cite xxx). This in fact results in a wild growing,
complex and heterogeneous data landscape that we need to deal with. The
application of ontologies in ecology is thus discussed controversially (cite
xxx) and it is argued that they can be a benefit, but it is hard to set up a
sophisticated ontology covering all necessary terms and relation of a that
complex research domain like ecology (cite xxx).

With growing global data pool there is a growing demand to use and reuse
available data and to embed small heterogeneous data into a wider context. We
here introduce the R package `rbefdata` that in combination with `BEFdata`
exactly deals with that. We showcase the functionality of the package available
with version 0.3.5 creating a workflow that integrates two datasets using a
analysis that has been published already and discuss the `rbefdata` package and
`BEFdata` in the light of upcoming developments like the integration of an
ontology we built that will make finding data and smart merges possible to help
researchers to deal with the upcoming challenges in handling complex and
heterogeneous data.

## Material and Methods 

### BEF-China and the BEFdata portal

The BEF-China experiment is a Biodiversity Ecosystem Functioning (BEF)
experiment funded by the German science foundation (DFG, FOR 891). It is
located in the subtropics of China in the provinces Jianxi and Zhejiang. The
BEF-China research group (www.bef-china.de) uses two main research platforms.
An experimental forest diversity gradient of 50~ha, and 27 observational plots
of 30x30 m each located in the Gutianshan Nature Reserve.  The observational
plots were selected according to a crossed sampling design along tree species
richness and stand age. The data for the workflow on carbon pools stems from 22
to 116 years consisting of 14 to 35 species (cite Bruelheide, 2010).  

The [BEFdata](http://befdataproduction.biow.uni-leipzig.de/) portal is an open
source data management platform developed within the BEF-China project. It
adheres to standards like the Ecological Metadata Language for describing
datasets with metadata and is specialized in harmonizing small heterogeneous
data that usually has to be dealt with in BEF. But its specialization makes it
also very valuable to use in any other scientific domain that needs to deal
with complex small and heterogeneous data. 

The portal offers a social component where researchers can shop datasets and
write a paper proposals based on the datasets in the shopping cart. In the
process of creating a proposal some information like a title, a rationale, an
envisaged journal and date needs to be provided. Sending in a proposal a
researcher asks for access to the datasets and provides the data owners with
necessary information about the paper. The data owners then can decide if and
how they like to participate in the upcoming paper or if they only like to get
acknowledged for providing their data (cite Karin).

### The proposal

We use an already published dataset as an example to present the
functionalities and inter linkages between the BEF-China data portal and
`rbefdata`. To test the effect of species richness on system N retention and
tree sapling N uptake we conducted a 15N tracer experiment in a young tree
plantation. To this end, saplings of four abundant early successional tree
species have been planted in monoculture, in two- and four-species mixtures,
and as individual trees. Afforestations are increasing globally to produce
timber and pulp wood, but also to enhance ecosystem services such as carbon
sequestration, nutrient retention, or groundwater recharge. In order to further
optimise these services with regard to balanced nutrient (particularly
nitrogen) cycles, it is important to know whether the use of mixtures of native
tree species in afforestation projects promotes greater acquisition and
retention of nitrogen compared to the currently established large-scale
monoculture. 

Four species were chosen for the experiment: Schima superba Gardn. et Champ.
and Elaeocarpus decipiens Hemsley (evergreen), Quercus serrata Murray and
Castanea henryi (Skan) Rehd. et Wils. (deciduous; Yu et al. 2001). The
following planting schemes were established in 1-mÂ² plots.  In each plot 16
saplings were planted in an array of four by four. Monocultures, two-species
combinations and four-species combinations were established. The four study
species provided a total of eleven species combinations four monocultures, six
two-species combinations, and one four-species combination. All treatments were
replicated four times, once in each of the four blocks. Pulse labelling with
15NH415NO3 (98% 15N) was performed in August and in September 2009. Leaf, fine
root and soil samples have been collected in September 2010. Samples have been
analysed for 15N content and leaf, fine root and soil recovery have been
calculated. The sum of the three compartment recoveries is referred to as
system N retention.  Relative leaf, root and soil recovery was calculated as
percentage of system N retention (for a detailed description of the material
and methods we refer to Lang et al. 2013). 

* figure shows the proposal page

![showcase_proposal](./figure/static/showcase_proposal.png)

### rbefdata 

The `rbefdata` package started its development within the BEF-Cina experiment.
Meanwhile it is part of the rOpenSci package portfolio (http://ropensci.org/),
which is a community driven approach to wrap all science APIs and to create
solutions to pull data from different repositories into R for analysis.  The
package can be installed from the CRAN package repository
(https://github.com/befdata/befdata) and enables access to the data, meta data
structures of the platform and provides convenient methods to pull single or
multiple dataset into the R environment in one step for analysis.  Additionally
it offers functions that help to upload final results datasets with the script
attached that has been used to derive the results from the original datasets
which provides a valuable insight into data provenance and also is a stepping
stone for reproducible research.

## Usecase (results)

The next step after an accepted paper proposal is to setup the `rbefdata`
package. This requires loading the package and setting the required package
options. Having a look into the options list reveals several fields that can be
filled in, like the URL to the BEFdata server, user credentials and a download
folder name that is used to store downloaded freeformat files that can be
attached to datasets. The tematres server related URLs in the options are part
of upcoming features that are non fully functional on the time of writing and
thus can be ignored. 

The most essential setting for the workflow we present here is the user
credentials. These are used to authenticate the user against the portal to
ensure the access to the data has been granted before download and to log the
data access. Setting the server URL is not required here as it defaults to the
BEF-China project instance of the BEFdata portal that we retrieve data from. If
one has set up an own instance of the BEFdata portal, this URL needs to be
changed so the package communicates with the right server (see box below).

```{r require_and_setup}
require(rbefdata)  
# options list
bef.options() 

# querry single options
bef.options("url") 
```

```{r require_and_setup_two}
# set credentials example
bef.options("user_credentials" = "aölkjspoiul12")  
```

```{r require_and_setup_three, eval = F, include = T}
# set URL example
bef.options("url" = "http://my.own.befdata.instance.com")
```

```{r load_secret_credentials_and_setup, eval = T, include = F}  
# load your credentials
source("secrets.R") 
```

After the setup of `rbefdata`  one can start right away using data from the
proposal. The proposal download function is used for that which draws all
associated datasets of a proposal into the R environment in one single step
(see blow). The function requires the ID of the proposal which can be found in
the end of the URL of the proposal in the BEFdata portal (see blow).

```
# the id is 90
http://befdataproduction.biow.uni-leipzig.de/paperproposals/90
```

```{r rbefdata_get_datasets_from_proposal, cache = T}  
# proposal id is 
datasets = bef.get.datasets_for_proposal(id = 90)
extract_one_dataset = datasets[[1]]
```

The metadata in EML format provided by the `BEFdata` portal is also accessible
in `rbefdata`. Each dataset on download is associated with the metadata
provided by its authors. This information can be extracted using the built-in R
function `attributes()`. As this requires granted access rights to the dataset,
there is also a function to only draw metadata of a dataset which is always
free for download (see code box below). 

```{r rbefdata_metadata} 
# extract title of one dataset
attributes(datasets[[1]])$title  

# for all dataset in the proposal
titles = sapply(datasets, function(x) attributes(x)$title)
titles 

# other metadata available
names(attributes(datasets[[1]])) 

# only get metadtata by dataset id
bef.portal.get.metadata(dataset = 335)$title
``` 

We write the datasets into two variables called `Nretention` and `design`
before merging. Both datasets contain a column with a `plot_id` that has been
used for merging. After merging the new synthesis dataset still contains many
unused column that will be removed in another step so only variables of
interest remain.

To analyse the dataset of system N retention we do need information about
species diversity of the plots and the information about which plot is placed
in which block from the design dataset. Furthermore we need values of the
initial basal diameter from the dataset `Nrecov` 

!!! Note I cannot find Nrecov dataset where is that one you are talking about?

```{r anne_script_seprate_datasets} 
# extract into separate datasets
Nretention = datasets[[1]]
design = datasets[[2]]

# overview about the contents of the datasets
names(Nretention)
names(design) 
```

The response variables have been checked for normality with `qqplot` and
transformed where necessary (box below).

```{r anne_script_two} 
# the synthesis dataset
syndata = merge(Nretention, design)

# overview about the content of the synthesis dataset
names(syndata)
syndata=syndata[-c(9:14,16:41)]
names(syndata)

### check for data properties (keep?)
str(syndata)

### we want to use 'species_diversity' as a factor

syndata$species_diversity=as.factor(syndata$species_diversity)
attach(syndata)

### All response variables have been square root transformed

syndata$perroot_plot_t=syndata$perroot_plot^.5
syndata$persoil_plot_t=syndata$persoil_plot^.5
syndata$recov_plot_t=syndata$recov_plot^.5
syndata$perleaf_plot_t=syndata$perleaf_plot^.5
```

We analysed our data by linear mixed effects models. Since the plots are
clumped in space, we use block as a random factor we will use the R packages
(`nlme`) for model analysis and (`multcomp`) for post-hoc comparisons

!!! What is the package "car" used for?

```{r anne_script_load_packages, echo = F} 
library(nlme)
library(multcomp)
library(car)
```

* To adjust for the unbalanced experimental design, Anova Type II (package “car” (Fox & Weisberg 2011)) was used to test for main effects 

```{r anne_script_model_tests}
### Model one Overall recovery/N retention  
model1 = lme(recov_plot_t~gbd_T0.mm.+species_diversity,syndata,random=~1|block,na.action=na.omit,method="REML")
anova(model1)
summary(glht(model1,linfct = mcp(species_diversity="Tukey"))) 

# ANOVA type II test for unbalanced design
model1c=Anova(model1,type="II" )
model1c

# model evaluation
# plot(model1,resid(.)~fitted(.))
# plot(model1,recov_plot_t~fitted(.))

## Model2 percentage leaf recovery of plot recovery 
model2=lme(perleaf_plot_t~species_diversity,syndata,random=~1|block,method="REML")
anova(model2)
summary(glht(model2,linfct = mcp(species_diversity="Tukey")))
Anova(model2,type="II")

# model evaluation
# plot(model2,resid(.)~fitted(.))
# plot(model2,perleaf_plot_t~fitted(.))

## Model3 percentage root recovery of overall recovery
model3 = lme(perroot_plot_t~species_diversity,syndata,random=~1|block,method="REML")
anova(model3)
summary(glht(model3,linfct = mcp(species_diversity="Tukey")))
Anova(model3,type="II")

# model evaluation
# plot(model3,resid(.)~fitted(.))
# plot(model3,perleaf_plot_t~fitted(.))

## Model 4 percentage soil recovery of overall recovery
model4 = lme(persoil_plot_t~species_diversity,syndata,random=~1|block,method="REML")
anova(model4)
summary(glht(model4,linfct = mcp(species_diversity="Tukey")))
Anova(model4,type="II")

# model evalution
# plot(model4,resid(.)~fitted(.))
# plot(model4,perleaf_plot_t~fitted(.)) 
```

```{r final_plot, echo = F}
attach(syndata)
par(mfrow=c(2,2),bty="l")
boxplot(recov_plot~species_diversity,las=1,ylab="N retention (at.%)",col="gray",
        xlab="Species richness")
text(3.4,31,"(A)",font=2)
boxplot(perleaf_plot~species_diversity,las=1,ylab=" Relative leaf recovery (%) ",
        col=c("dark gray","white","light gray"),xlab="Species richness")
text(1,25,"b",font=2);text(2,25,"a",font=2);text(3,25,"ab",font=2);text(3.4,27,"(B)",font=2)
boxplot(perroot_plot~species_diversity,las=1,ylab="Relative root recovery (%)",
        col=c("dark gray","white","white"),xlab="Species richness")
text(1,8.3,"b",font=2);text(2,8.3,"a",font=2);text(3,8.3,"a",font=2);text(3.4,8.3,"(C)",font=2)
boxplot(persoil_plot~species_diversity,las=1,ylab="Relative soil recovery (%)",
        col=c("white","light gray","dark gray"),xlab="Species richness")
text(1,70,"a",font=2);text(2,70,"ab",font=2);text(3,70,"b",font=2);text(3.4,98,"(D)",font=2)
```

* caption: 

## Discussion

* General discussion 
  - high need to effective use/reuse data 
  - relevant data needs to be simply detectable 

* BEFdata and rbefdata 
  - in combination provide a solution to 
    + data storage
    + describing data with metadata
    + collaboration and data sharing 
    + simply pull data into analysis software and push data back
    + data provenance by attaching R scripts to uploads
  - will provide solution with next versions 
    + easier finding relevant data 
    + smart merges (including unit conversions)

As there is a growing demand to effectively reuse available data this puts much
pressure on the development of solutions that help researchers not only to find
but also to integrate heterogeneous small data into a wider context in
different analyses (cite xxx, data intensive science, long tail). The
combination of `BEFdata` and the `rbefdata` package provides a solutions to a
one part of the data life cycle and especially introduces a solution to deal
with high heterogeneous data.

We recently stared to develop an ontology using a `tematres` server containing
knowledge extracted from portals that deal with data management for ecological
research. The `tematres` server offers an API so all the contained terms can be
accessed by the upcoming version of `rbefdata` 

The formalization developed is and will be based on the knowledge used in
biodiversity research. Thus we will here discuss the software combination
`BEFdata` and `rbefdata` in the light of the upcoming features and in general
context state of the art data management today. In one of the next versions to
be rolled out the `BEFdata` portal will get a semantical annotation feature.
This will give administrators the ability to tag each column of datasets with a
general term that best describes the content. So the field will contain
potential top terms of the ontology. The tagging will be reflected in the API
and can thus be simply queried to use the information within the R package.
Using the knowledge about the content of a column in the R package will enable
us to do support smart merges that work.

`tematres` ([homepage](http://www.vocabularyserver.com/))into BEFdata and the
rbefdata package so they play well together semantically.

## Acknowledgements 

Thanks to all the data owners of the proposal for providing access to the
datasets. ...

## Literature

This will be done externally as markdown has no good way to deal with
references and stuff. We can collect here a list of links to references. I will
then collect them with e.g zotero to export a format we can hand them in for
publication.

## Appendix

* maybe will not be used extensively but we will see

### Figures 

* vizualization plugin (keywords) 

One can visualize the keywords associated with the dataset of a BEFdata portal
using the vitalization functionality. This gives a short overview about the
contents the portal data is dealing with.


```{r vizalize_keywords, warings = F, messages = F, errors = F, echo = F} 
bef.portal.vizualize.keywords()
```

### Tables
