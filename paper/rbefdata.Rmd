```{r load_requirements, eval = T, include = F}  
# this loads the befdata package
require("rbefdata")  
require("ggplot2") 
```

## Abstract 

The story board:

* Huge amount of data available (deluge of data)
* More and more gets accessible (with some hindrances) 
    - metadata missing (renders data useless) 
    - lack of reward for data providers (people keep their data secret)
    - so lots of data gets still lost (needs help e.g rebind)
* Data often hard to find/reuse 
    - if not well described 
* But there is a growing need to reuse data 
    - so we need tools supporting researchers in doing so (reuse available)
* The BEFdata portal provides a good base for heterogeneous data in ecology  
    - we dock onto that with rbefdata to pull data into data statistics  
    - Data life cycle (we cover it partially focused and doing this well) 
        + BEFdata (Store data, Describe it with metadat, Collaborate and Share it)
        + rbefdata (Find and understand and analyse data as well as reuse of data). 
          
* Maybe also shorlty introduce the data lifecycle and use it to show to which 
  parts of that cycle we offer soutions.

## Introduction 

* data is valuable 
    - also in the long run  
    - there are solutions for data storage
    - and also nodes that collect the data 

With a growing awareness on the long term value of data, much effort has been
put into building data management platforms, to preserve all kind of
environmental and historic data, over the last years (e.g. diversity workbench,
BEFdata). Many specialized solutions for different scientific disciplines
appeared that provide data management plans for small scale projects or
collaborations as well as for large data producing long term or remote sensing
projects. An ongoing trend in that context is the development of integrative
databases or data portals. They serve as nodes that collect data from smaller
databases of a certain domain and they give researchers of that domain the
opportunity to access a wide range of relevant data from one place.  This
portals in fact offer a solution to to one of the most pressing problems that
we face with our valuable data today, their lost. 

* data reuse 
    - data needs to be reusable 
    - otherwise it is almost worthless
    - this needs metadata so others can understand what we have done

Another big problem with data, especially in terms of reuse of available data,
is the general understanding of datasets. Usually plain datasets say nothing,
to one who is not familiar with it and they are even hard to decipher by the
author itself after some time has passed. It is usually hard to remember
exactly what methods have been used to collect a certain columns data or what
the abbreviations or headers in the dataset mean. To solve this this problem
metadata frameworks have been developed and published as standards so nobody
really needs to think about an own set of requirements to describe its data.
The Ecological Metadata Language is only one example for that. While this
theoretically solves the problem with not well described datasets it is still
hard to make people use it extensively as this usually always means to learn
new tools that help with the description process.

* find/merge and prepare data takes time   
    - ontologies are discussed controversially in ecology 
    - on one hand they offer a solid backbone to develop smart software
    - on the other hand they are they are hard to create especially 
      for a high heterogeneous reaearch domain like ecology.

While well described data helps a lot in understanding datasets and on deciding
upon the relevance and applicability in a certain analysis there is still lots
of manual intervention necessary after that to prepare the data for analysis.
It may needs to be cleaned, imputed, reshaped and merged which usually takes up
to 70% of the analysis workflow, before the smart models can be applied to the
data to find interesting patters (cite the workflow paper of Karin and me).
This preparation steps are not only time and labour intensive but also
potentially error prone, especially as the complexity of analyses grows. 

* Ontologies are either already used or discussed controversially 
  - ontologies are used to capture knowledge as formal representations of domain knowledge
  - they can be used to develop smart software 
  - smart software can help 
    + in finding relevant data 
    + merge and prepare heterogeneous data for analysis

Ontologies, formal representations of knowledge potentially offer a
sophisticated tool to deal with that step of data preparation. While they are
already used in some research domains like genetics, other domains face more
problems using it. For example in ecology, that has grown into a very
collaborative, interdisciplinary and data intensive science over the last
decade, to address questions on a greater temporal and spatial scale (e.g
michener et al 2012). The data here is mainly provided by small scale studies
spread all over the world (e.g heidorn2009 shedding light on the dark) but also
through bigger long term projects like LTER (cite xxx), BEF-China (cite xxx),
governmental projects and local initiatives (cite xxx). This in fact results in
a wild growing, complex and heterogeneous data landscape in that we need to
deal with. The application of ontologies in ecology is discussed
controversially (cite xxx) which is mainly related to the heterogeneity of the
research domain and it is argued that they can be a benefit but it is hard to
set up a versatile ontology covering all necessary terms of a complex research
domain like ecology.

* rbefdata in combination with BEFdata provide solutions to a parts of the data
  life cycle in ecology:
    - store data 
    - describe the data (metadata) 
    - share data and 
    - find relevant data 
    - get the data into Statistics software 

As there is a growing demand to use and reuse available data and to embed small
heterogeneous data into a wider context in ecology we here introduce the R
package `rbefdata` that deals exactly with that. We showcase the functionality
of the package available with version 0.3.5 creating a workflow for integration
of two datasets and discuss the rbefdata package and BEFdata in the light of
future development on the integration of ontologies that will make finding
data, smart merges and unit conversion possible to help researchers to deal
with the upcomming challenges in dealing with data like integration of
heterogeneous datasets.

## Material and Methods 

### BEFdata portal

The BEFdata portal (cite Kain,
[url](http://befdataproduction.biow.uni-leipzig.de/)) is an open source data
management platform developed within the BEF-China project (FOR 891) which is
funded by the German science foundation. The BEF-China experiment is a
Biodiversity Ecosystem Functioning experiment in the subtropics of China
situated in the provinces of xxx and yyy ( do we really need to go into project
details )

The BEFdata platform sticks to standards like the Ecological Metadata Lanauge
for metadta and is specialized in harmonizing small heterogeneous data that
usually has to be dealt with in biodiversity ecosystem functioning but also in
other parts of ecological research. The portal offers a social component that
lowers the hurdles on sharing data online and tools that help researchers to
describe their data with metadata. 

As a paper proposal process integrated. You shop together datasets and
afterwards create a paper proposal based on the shopped dataset. In the
proposal you have to give information like a title for the proposal and a
rationale describing how you intend to use the data and where and when to
publish the results. If the proposal is handed in the authors will be informed
that somebody likes to access their datasets and they can decide if they like
to participate and how.  After all authors have granted access on is good to go
with the `rbefdata` package. 

### Data used

The data used for the presentation of this package stems from (A. Lang. ...)
from the BEFdata portal. The data is free as it already was published.

### rbefdata

The rbefdata package is the companion to the open source data management
solution BEFdata that has been developed and is used within the BEF-China
project (FOR xxx). The package provides access to the data and the meta data
stored on a BEFdata portal instance and offers convenient methods to pull
multiple dataset into the R environment in one step for analysis as well as
functions that help to upload final results as datasets with a the script
attached that has been used to derive the results from the original data pulled
from the portal. This is not only a good star

We show a worfkow for the integration of two datasets starting from a paper
proposal within the BEFdata platform. The analysis shown is based on alredy
published data and will only be there for the reason to show a real usecase.

and scripts that have been used to derive the results from the original
datasets which offers an insight into data provenance and also is a stepping
stone for reproducable research.

We showcase the package building a workflow that uses already published data.
It shows how to pull data and metadata into the R environment for analysis and
how the data can be put back directly after the analysis with an attached
script which is good for data provenance. 

The `rbefdata` package offers an option list that is used to determine the
servers URLs the package contacts to to retrieve data, the download folder name
for attached free format content of a dataset and 

### Setup rbefdata 

* load the package

```{r require_rbefdata}
require(rbefdata)
```

* list options

```{r list_rbefdata_options}
bef.options()
```

* query options

```{r query_rbefdata_options}
bef.options("url")
```

* set options 

```{r set_rbefdata_options, eval = F}
bef.options("user_credentials" = "a√∂lkjspoiul12")
bef.options("url" = "http://my.own.befdat.instance.com")
```

## Results (or rather usecase)

```{r load_secret_credentials_and_setup, eval = T, include = F}  
# load your credentials
source("secrets.R") 
```

For the workflow we start right after the access to the data has been approved
by all data owners. We use the ID of the proposal to pull all datastes in one
step into the R statistics environment (see below). 

```{r rbefdata_get_datasets_from_proposal, cache = T} 
require(rbefdata)
dataset_list = bef.get.datasets_for_proposal(id = 90)
extract_one_dataset = dataset_list[[1]]
```

* Inspect datasets

The BEFdata portal offers metadat in Ecological Metadata Language format
standard for download (cite EML). We make use of that metadata in the`rbefdata`
package as well and each dataset is associated with its metadata on download.
So you always have acces to the information that is required to understand a
dataset. This information can be extracted from a dataset with the R command
`attributes()`

```{r rbefdata_metadata}
attributes(dataset_list[[1]])$title
``` 

* write your scripts (showcase here comes annes input)


## Discussion

Discussion will be in the light of future features like the upcomming
integration of tematres into BEFdata and the rbefdata package so they play well
together semantically.

## Appendix

### Figures 

* vizualization plugin (keywords) 

```{r vizalize_keywords, cache = T, waring = F, message = F, error = F} 
bef.portal.vizualize.keywords()
```

### Tables
