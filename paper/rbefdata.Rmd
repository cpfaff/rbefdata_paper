```{r load_requirements, eval = T, include = F}  
# this loads the befdata package
require("rbefdata")  
require("ggplot2") 
```

## Abstract 

The story board:

* Huge amount of data available (deluge of data)
* More and more gets accessible (with some hindrances) 
    - metadata missing (renders data useless) 
    - lack of reward for data providers (people keep their data secret)
    - so lots of data gets still lost (needs help e.g rebind)
* Data often hard to find/reuse 
    - if not well described 
* But there is a growing need to reuse data 
    - so we need tools supporting researchers in doing so (reuse available)
* The BEFdata portal provides a good base for heterogeneous data in ecology  
    - we dock onto that with rbefdata to pull data into data statistics  
    - Data life cycle (we cover it partially focused and doing this well) 
        + BEFdata (Store data, Describe it with metadat, Collaborate and Share it)
        + rbefdata (Find and understand and analyse data as well as reuse of data). 
          
* Maybe also shorlty introduce the data lifecycle and use it to show to which 
  parts of that cycle we offer soutions.

## Introduction 

* data is valuable 
    - also in the long run  
    - there are solutions for data storage
    - and also nodes that collect the data 

With a growing awareness on the long term value of data, much effort has been
put into building data management platforms, to preserve all kind of
environmental and historic data, over the last years (e.g. diversity workbench,
BEFdata). Many specialized solutions for different scientific disciplines
appeared that offer data management plans for small scale projects or
collaborations as well as for large data producing long term or remote sensing
projects. An ongoing trend in that context is the development of integrative
databases or data portals. They serve as nodes that collect data from smaller
databases of a certain domain and they give researchers of that domain the
opportunity to access a wide range of relevant data from one place.  This
portals in fact offer a solution to to one of the most pressing problems that
we face with our valuable data today, their lost. 

* data reuse 
    - data needs to be reusable 
    - otherwise it is almost worthless
    - this needs metadata so others can understand what we have done

Another big problem with data, especially in terms of reuse of available data,
is the general understanding of datasets. Usually plain datasets say nothing,
to one who is not familiar with it and they are even hard to decipher by the
author itself after some time has passed. It is usually hard to remember
exactly what methods have been used to collect a certain columns data or what
the abbreviations or headers in the dataset mean. To solve this this problem
metadata frameworks have been developed and published as standards so nobody
really needs to think about an own set of requirements to describe its data.
The Ecological Metadata Language is only one example for that. While this
theoretically solves the problem with not well described datasets it is still
hard to make people use it extensively as this usually always means to learn
new tools that help with the description process.

* find/merge and prepare data takes time   
    - ontologies are discussed controversially in ecology 
    - on one hand they offer a solid backbone to develop smart software
    - on the other hand they are they are hard to create especially 
      for a high heterogeneous reaearch domain like ecology.

While well described data helps a lot in understanding datasets and on deciding
upon the relevance and applicability in a certain meta analysis there is still
lots of manual intervention necessary after that to prepare the data for
analysis. It may needs to be cleaned, imputed, reshaped and merged which
usually takes up to 70% of the analysis workflow, before the smart models can
be applied to the data to find intresing patters (cite the workflow paper of
Karin and me). This preparation steps are not only time and labour intensive
but also potentially error prone, especially as the complexity of analyses
grows. Ontologies, formal representations of knowledge potentially offer a
versatile tool to deal with that step of data preparation.

* Ontologies are discussed controversally 
    * Here we like to introducde you to the rbefdata package
        - it provides a solution to one part of the dat lifecycle
        - together with the BEFdata portal it covers: 
            + data storage 
            + harmonization 

While they are already heavily used in some research domains like genetics
other domains face more problems with them. For example, ecology has grown into
a more collaborative, interdisciplinary and data intensive science over the
last decade, to address questions on a greater temporal and spatial scale (e.g
michener et al 2012). The data here is mainly provided by small scale studies
spread all over the world (e.g heidorn2009 shedding light on the dark) but also
through bigger long term projects like LTER (cite xxx), BEF-China (cite xxx),
governmental projects and private initiatives (cite xxx). This in fact results
in a wild growing, complex and heterogeneous data landscape in that we need to
deal with. The application of ontologies in ecology is discussed controversally
(cite xxx) which is mainly related to the heterogeneity of the research domain
and it is argued that they can be a benefit but it is hard to set up a
versatile ontology covering all necessary terms of a a complex research domain
like ecology.

* provide solution to a part of the data lifecylcle 
  - find relevant data 
  - Pul it into analysis software 

There is a growing need to make use of the data available and we need to think
of ways on how to make the most out of the data we alredy have before we start
to collect more data in general or even worse repeat to pick up data that
already has been collected by someone else.

Another problem with data is t putting data into a public accssable database is
a good step for long term perservation this is only the first step. 

While some research disciplies do very well with the solutions available (e.g
Genetics), others facing more problems. 


The integration of dataset into meta analyses is still a very time and labour
intensive process. 

which is
error prone especially with a growing heterogeneity of the onderlying data.

Ontology needs also be discussed here a bit more intensive... 

Ontologies can play an important role as they potentially offer backbones for
the development of smart tools helping researchers to find and merge data (cite
XXX). But their application in ecology is discussed controversially as
capturing the knowledge of ecology in a formal representation can be highly
complex for a interdisciplinary research domains. 

We here show how heterogeneous data from the BEF-China project is integrated
into a workflow using the combination of the BEFdata portal and its companion
package `rbefdata`. We use data that is already published and open access so
everybody will be able to reproduce this analysis.

## Material and Methods 

### BEFdata portal

The BEFdata portal (cite Kain,
[url](http://befdataproduction.biow.uni-leipzig.de/)) is a data management
platform developed within the BEF-China project (FOR 891) of the German science
foundation. It is specialized in harmonizing small heterogeneous data, offers a
social component that lowers the hurdles on sharing data online and tools that
help researchers to describe their data with metadata.

### Data used

The data used for the presentation of this package stems from (A. Lang. ...)
from the BEFdata portal. The data is free as it already was published.

### Rbefdata

The `rbefdata` package offers an option list that is used to determine the
servers URLs the package contacts to to retrieve data, the download folder name
for attached free format content of a dataset and 

* load the package

```{r require_rbefdata}
require(rbefdata)
```

* list options

```{r list_rbefdata_options}
bef.options()
```

* query options

```{r query_rbefdata_options}
bef.options("url")
```

* set options 

```{r set_rbefdata_options, eval = F}
bef.options("user_credentials" = "a√∂lkjspoiul12")
bef.options("url" = "http://my.own.befdat.instance.com")
```

```{r load_secret_credentials, eval = T, include = F}  
# load your credentials
source("./secrets.R") 
```

Introduction to the rbefdata R package

* get datasets 

In the very heart of the BEFdata portal there is a paper proposal process
integrated. You shop together datasets and afterwards create a paper proposal
based on the shopped dataset. In the proposal you have to give information like
a title for the proposal and a rationale describing how you intend to use the
data and where and when to publish the results. If the proposal is handed in
the authors will be informed that somebody likes to access their datasets and
they can decide if they like to participate and how. After all authors have
granted access on is good to go with the `rbefdata` package. One can draw all
datasets associated to a proposal in one turn with the package.

```{r rbefdata_get_datasets_from_proposal}
dataset_list = bef.portal.get.datasets_for_proposal(id = 1)
extract_one_dataset = dataset_list[[1]]
```

* Inspect datasets

The BEFdata portal offers metadat in Ecological Metadata Language format
standard for download (cite EML). We make use of that metadata in the`rbefdata`
package as well and each dataset is associated with its metadata on download.
So you always have acces to the information that is required to understand a
dataset. This information can be extracted from a dataset with the R command
`attributes()`

```{r rbefdata_metadata}
dataset_list = bef.portal.get.dataset_for_proposal(id = 1) 
attributes(dataset_list[[1]])$title
```
* write your scripts 


* vizualize the portal (keywords) 

see figure ... in appendix

## Results

The results presents a usecase maybe we just rename results to usecase and
introduce the data used in the material and methods section. 

## Discussion

Discussion will be in the light of future features like the upcomming
integration of tematres into BEFdata and the rbefdata package so they play well
together semantically.

## Appendix

### Figures 

```{r vizalize_keywords, cache = T, waring = F, message = F, error = F} 

bef.portal.vizualize.keywords()
```
### Tables
