```{r load_requirements, eval = T, include = F}  
# this loads the befdata package
require("rbefdata")  
require("ggplot2")  
options(width = 95)
```

## Abstract

Today we face a deluge of data, scientists need to deal with in many different
disciplines. While there are already good software solutions to assist
researcher throughout the data life cycle, the applicability of the solutions
to certain scientific domains often varies. We here introduce the `rbefata` R
package that connects to the open source data management platform `BEFdata`
that has been developed and is used within the BEF-China experiment. We show
the use of the package and its interaction with the data management platform
using an example workflow that integrates two datasets from the BEF-China
experiment. The analysis in the workflow is representing already published
results, so the data will be open access in the near future. Finally we discuss
the introduced combination of software in the context of the data life cycle
and current as well as future data management requirements. Additionally we
give an outlook on upcoming semantical features like an assisted search and
smart merging functionality to be integrated with `rbefdata` and `BEFdata`.

## Introduction

With a growing awareness on the value of data, much effort has been put into
building data management platforms, to preserve all kind of environmental and
historic data, over the last years (e.g. diversity workbench, GBIF, `BEFdata`,
DataONE, LifeWatch). Many solutions for different scientific disciplines
appeared that provide data management plans for small scale projects or
collaborations as well as for large data producing, long term or remote sensing
projects. An ongoing trend in that context is the development of integrative
databases or data portals. They serve as nodes that collect data from smaller
databases of a certain domain and they give researchers of that domain the
opportunity to access a wide range of relevant data all from one place (e.g
GBIF, TRY). These data management portals in fact offer a solution to to one of
the most pressing problems that we face with our valuable data today, their
loss.

However, by developing tools that allow an easy access and analyses of these
data, it can be ensured that these are not only preserved but also used, reused
and embedded into into a wider context. A problem here is the legibility of
datasets. Usually plain datasets say nothing, to one who is not familiar with
it and they are even hard to decipher by the author himself after some time has
passed. It is usually hard to remember exactly what methods have been used to
collect a certain columns data or what the abbreviations or headers in the
dataset mean. To solve this problem metadata frameworks have been developed and
published as standards so nobody really needs to think about an own set of
requirements to describe their data. The Ecological Metadata Language
[EML](http://knb.ecoinformatics.org/software/eml/) is only one example for
that. While this theoretically solves the problem with not well described
datasets it is still hard to make researchers use it extensively as this
usually always means to learn new tools that help with the description process
(e.g morpho, data up).

In this paper we want to introduce the new R package `rbefdata` which links the
open source data management platform `BEFdata` portal with the statistics
environment R. The `rbefdata` package is part of the rOpenSci initiative which
aim is to give the R - Users all the flexibility of data access through APIs.
The aim of the package is to provide the tools to download data, analyse them
and upload them back to the data portal. Additionally it offers methods that
enable the exploration of datasets by keyword associations and an integration
of a vocabulary server which can assists the dataset exploration process. 

We showcase the functionality of the package available with version `0.3.5`
creating a workflow that integrates two datasets. The use case is dealing with
data from a small scale experiment called pilot experiment in the BEF-China
experiment. It is a 15N tracer experiment which aims to disentangle the effect
of species mixtures on system N retention. The workflow depicts how to pull
data into the R environment, the inspection of datasets metadata and how to
upload data and attachments. It reconstructs a facet of an analysis that has
been published already (Lang et al. 2013). 


## Material and Methods

### BEF-China 

The BEF- (Biodiversity and Ecosystem Functioning) China project is a research
group whose aim is to disentangle the 'The role of tree and shrub diversity for
production, erosion control, element cycling, and species conservation in
Chinese subtropical forest ecosystems'. The BEF-China research group
(www.bef-china.de) is funded by the German science foundation (DFG, FOR 891)
and uses two main research platforms located in the provinces Jiangxi and
Zhejiang.  The Main experiment was established in 2009 and 2010 as the first
large scale forest biodiversity–ecosystem functioning (BEF) experiment in the
highly species-rich subtropics.  In total, the area covers 50 ha and there are
566 plots of 400 trees each, ranging in diversity from monoculture to
24-species mixtures. The experiment used 42 native tree species and 10 shrub
species, combined into different species pools. More than 400 000 tree and
shrub saplings were planted.  In a parallel observational approach, a total of
27 Comparative Study Plots (CSPs) of 30x30 m each were set up in existing
forests in the adjacent Gutianshan National Nature Reserve (Zhejiang Province)
in 2008.  The observational plots were selected according to a crossed sampling
design along tree species richness and stand age. The CSPs address the impact
of successional age on ecosystem functioning, providing a basis for assessing
the successional processes at work across tree species diversity in the Main
Experiment (Bruelheide et al., 2012). 

### BEFdata portal

The [BEFdata](http://befdataproduction.biow.uni-leipzig.de/) is specialized in
managing small and heterogeneous datasets. It adheres to standards like EML and
offers a social component as well. The portal facilitates research cooperation
by the tool of paper proposals.  Sending in a proposal, a researcher asks for
access to the datasets and proposes his research idea to the data owners and
possible collaborators. This more social component of the portal allows to
include and acknowledge all researchers involved in the data sampling process,
promotes collaborations between research units, avoids publication initiatives
of the same research ideas and adds to the transparency of data publication.
To create a paper proposal the researcher can shop (select) datasets which are
to be included in the analyses.  Furthermore basic information of the proposed
paper such as the title, the rationale, the envisaged journal and date needs to
be provided. The data owners and proposed collaborators are informed and can
decide if and how they like to participate in the upcoming paper or if they
only like to get acknowledged for providing their data (cite Karin).
Furthermore the datasets assembled by the paper proposal can be readily
imported in one step to the R environment by `rbefdata`.

### The proposal

Starting on with a paper proposal on following rationale.  

We created a paper proposal with the following rationale: 'Knowledge of
biodiversity effects on nutrient cycling patterns in subtropical forest
ecosystems is still very limited, particularly as regards macro nutrients such
as nitrogen and phosphorus. Experimental approaches using tree saplings may
promote an understanding of mechanisms that underlie nutrient acquisition and
cycling in early successional stages of secondary forests and forest
plantations. Insights in the potential of nutrient retention of young tree
plantations are of particular interest in China, where large areas have been
reforested in order to counteract soil erosion and to increase the soils’ water
and nutrient retention capacity.  In this study we planted saplings of four
abundant early successional (evergreen and deciduous) tree species in
monoculture, two- and four-species combination to test the effect of species
richness on nitrogen acquisition and retention by using a 15N tracer
experiment. A crucial question in BEF research is the appropriate time scale of
experiments which allows species richness effects to emerge. This question
gains importance when long-lived and slowly growing organisms such as trees are
considered. We wanted to analyse whether species richness effects occur during
the establishment phase of early successional tree species typical of
subtropical forests of China.  More precisely we wanted to test the following
hypotheses: (H1) Nitrogen acquisition and retention increases with species
richness due complementary effects in species mixtures.  (H2) Species richness
effects strengthen over time.' The respective proposal can be assessed under
(url) For a detailed description of the experimental design we refer to Lang et
al. 2013 (DOI)

### rbefdata

The development of the `rbefdata` package started within the BEF-China project.
Meanwhile it is part of the rOpenSci package portfolio (http://ropensci.org/),
which is a community driven approach to wrap all science APIs and to create
solutions for R users to seamlessly pull data from different repositories
spread over the internet into R for analysis. The package can be installed from
the CRAN package repository (https://github.com/befdata/befdata) (see box
below). It enables access to the data and meta data structures of the platform
and provides convenient methods to pull single or multiple dataset into the R
environment in one step for analysis. Additionally it offers functions that
help to upload final results datasets with the script attached that has been
used to derive the results from the original datasets which provides a valuable
insight into data provenance and also is a stepping stone for reproducible
research.

```{r install_package, include = T, eval = F} 
install.packages("rbefdata")
```

## Example workflow (results)

In this paper we use an already published analysis as a use case to build a
workflow that shows the functionalities and inter linkages between the
BEF-China data portal and the `rbefdata` package. We start setting up the R
package, highlight the dataset exploration features including the vocabulary
integration over a `tematres` server, before we finally come to the paper
proposal and the download of data as well as to the upload of final results.

After loading `rbefdata` into the working environment the settings need to be
set via the `bef.options()` command. Having a look into the options list we see
several fields that can be filled in. The most essential setting for the
example workflow we present here is the user credentials. These are used to
authenticate the user against the portal and to ensure the access to the data
has been granted as well as to log the data access. We need this as the data is
not yet open access. But it will be in the near future and then a key is no
longer required to download the data.

Other options we see here are the URLs to the `BEFdata` instance and the
`tematres` server as well as a field that stores the name of a download folder.
While the URL fields ensure the package communicates with the right servers the
download folder name is used to create a folder in case we download attached
files from a dataset or proposal. In our case there is no need to change the
URL to `BEFdata` as it defaults to the BEF-China instance that we use to
retrieve data from. If one has set up an own instance of the `BEFdata` portal,
this URL needs to be changed so the package connects to the right server (see
box below).

```{r require_and_setup} 
# load the package
require(rbefdata)   

# list all options 
bef.options() 

# querry a single option
bef.options("url") 
```

```{r require_and_setup_two}
# set credentials example
bef.options("user_credentials" = "aölkjspoiul12")  
```

```{r require_and_setup_three, eval = F, include = T}
# set URL example
bef.options("url" = "http://my.own.befdata.instance.com")
```

```{r load_secret_credentials_and_setup, eval = T, include = F}  
# load your credentials
source("secrets.R") 
```

### Tematres vocabulary integration 

A `tematres` server can hold different representations of formalized knowledge
like a thesaurus or even an ontology of a project. The `rbefdata` package
supports exploiting a `tematres` vocabulary server via the `rtematres` package.
We can find terms and relations as well as we can display their descriptions in
`rbedfata`. This can be used to improve the exploration of datasets. For
example looking for datasets that deal with  "plant organs" we can display the
definition of that task first (box below). Then we ask the `BEFdata` portal for
datasets that are tagged with the keyword we are looking for. We get back a few
datasets. In a next step we use the vocabulary on the `tematres` server to
narrow down "plant organs" and we find "leaf",  "root", "twig" as well as
plural forms. We use the narrower keywords to query the `BEFdata` server again
for datasets associated with them.

```{r improved_data_exploration}   
# define the term plant organ
term_info = bef.tematres.api(task = "fetchNotes", "plant organ")  
term_info

# get datasets tagged with plant organ 
datasets_plant_organ = bef.get.datasets_for_keyword("plant organ")
datasets_plant_organ 

# narrow down plant organ
narrow_tasks_plant_organ = bef.tematres.api(task = "fetchDown", "plant organ")   
narrow_tasks_plant_organ  

# enrich the search with narrower keywords
datasets_plant_organ_narrow = bef.get.datasets_for_keyword(c(narrow_tasks_plant_organ$term)) 
datasets_plant_organ_narrow   
```

### Proposal workflow 

![showcase_proposal](./figure/static/showcase_proposal.png)

* caption: The paper proposal in its final approved state. The information on that page
           contains a title, a rational an envisaged date and journal. The calculated authors
           and email lists for communication as well as the attached datasets and sub
           projects involved (only partially shown). The proposal is published alredy see 
           (Lang et al. 2013).

If all data owners accepted the paper proposal `rbefdata` can be used to to
access the datasets. After setup we can start right away using data from the
proposal that we created. The proposal download function of `rbefdata` is used
for that. It draws all associated datasets of a proposal into the R environment
in one single step. It returns a list object that keeps a data frame per list
element containing a dataset of the proposal each (see blow).  The function
requires the ID of the proposal to work. The ID can be found in the URL of the
proposal (see box below).

```
# the proposal URL shows the id is 90 
http://befdataproduction.biow.uni-leipzig.de/paperproposals/90
```

```{r rbefdata_get_datasets_from_proposal, cache = F}
# proposal id is 
datasets = bef.get.datasets_for_proposal(id = 90)
extract_second_dataset = datasets[[2]] 
head(extract_first_dataset, 5)
```

Each dataset in the `BEFdata` portal is associated with metadata the authors of
the dataset provide. We also provide access to the metadata from within
`rbefdata`. It can be accessed either directly via a metadata download command
that takes the ID of a dataset or extracted via the R internal `attributes()`
command. The extraction via attributes is possible as each dataset is attached
with its metadata when using one of the download commands of `rbefdata` (see
box below).

```{r rbefdata_metadata} 
# get metadata only, by dataset ID
bef.portal.get.metadata(dataset = 335)$title

# extract title of first dataset in proposal
attributes(datasets[[1]])$title 

# extract all dataset titles in the proposal
titles = sapply(datasets, function(x) attributes(x)$title)
titles 

# other metadata available
names(attributes(datasets[[1]])) 
``` 

The dataset from the proposal contains three datasets, of which we do only use
the second and third. These two are written into two variables called
`Nretention` and `design` before deciding upon how to merge them. Inspecting
the headers of both dataset reveals each of them contains a column containing a
`plot_id` that seems suitable for merging. But we can also make use the
metadata for columns to check if this really is the case (see box below).

```{r anne_script_seprate_datasets} 
# extract into separate datasets
Nretention = datasets[[2]]
design = datasets[[3]]

# overview about the contents of the datasets

# names in dataset Nretention
names(Nretention) 

# description of column plot_id
Nretention_column_plot_id_description = attributes(Nretention)$columns[1,]$description
Nretention_column_plot_id_description 

# names in dataset Nretention
names(design)  

design_column_plot_id_description = attributes(design)$columns[4,]$description
design_column_plot_id_description 
```

After merging the datasets the new synthesis dataset still contains many
columns not required for the analysis that can be dropped. To analyse the
dataset of system N retention we need information about the species diversity
in the plots and about which plot is placed in which block from the design
dataset. 'Species diversity' is used as a factor containing three levels (1,2,4
species mixtures). The response variables have been checked for normality with
`qqplot` and transformed (see box below).

```{r anne_script_two} 
# the synthesis dataset
syndata = merge(Nretention, design)

# overview about the content of the synthesis dataset
names(syndata) 

# remove unwanted variables from synthesis datset
syndata=syndata[-c(9:14,16:41)]
names(syndata)

#> we want to use 'species_diversity' as a factor
syndata$species_diversity=as.factor(syndata$species_diversity)

# square root transforme response variables
syndata$recov_plot_t=syndata$recov_plot^.5 
syndata$perleaf_plot_t=syndata$perleaf_plot^.5 
syndata$perroot_plot_t=syndata$perroot_plot^.5
syndata$persoil_plot_t=syndata$persoil_plot^.5
```

We analysed our data by linear mixed effects models. Since the plots are nested
in blocks, we use block as a random factor. The analysis uses the R packages
`nlme` (Pinheiro et al. 2013) for modeling and `multcomp` (Hothorn et al. 2008)
for post-hoc comparisons. To adjust for an unbalanced experimental design an
ANOVA Type II was carried out to test for main effects using the R package
`car` (Fox and Weisberg 2011). The models have been evaluated visually.

```{r anne_script_load_packages, message = F} 
require(nlme)
require(multcomp)
require(car)
```

```{r anne_script_model_tests}
### Model 1: Overall recovery/N retention  
model1 = lme(recov_plot_t~gbd_T0.mm.+species_diversity,syndata,random=~1|block,na.action=na.omit,method="REML")
anova(model1)
summary(glht(model1,linfct = mcp(species_diversity="Tukey"))) 

# ANOVA type II test for unbalanced design
model1c=Anova(model1,type="II" )
model1c

### model evaluation
#checking plots to assess whether residuals are well behaved 
#plot(model1)
#whether the response variable is a reasonable linear function of the fitted values
#plot(model1,recov_plot_t~fitted(.))
#and whether the errors are reasonably close to normal distribution in all four blocks (Crawley 2009)
#qqnorm(model1,~resid(.)|block)

## Model2 percentage leaf recovery of plot recovery 
model2=lme(perleaf_plot_t~species_diversity,syndata,random=~1|block,method="REML")
anova(model2)
summary(glht(model2,linfct = mcp(species_diversity="Tukey")))
Anova(model2,type="II")

# model evaluation
#plot(model2)
#plot(model2,recov_plot_t~fitted(.))
#qqnorm(model2,~resid(.)|block)

## Model3 percentage root recovery of overall recovery
model3 = lme(perroot_plot_t~species_diversity,syndata,random=~1|block,method="REML")
anova(model3)
summary(glht(model3,linfct = mcp(species_diversity="Tukey")))
Anova(model3,type="II")

# model evaluation
#plot(model3)
#plot(model3,recov_plot_t~fitted(.))
#qqnorm(model3,~resid(.)|block)

## Model 4 percentage soil recovery of overall recovery
model4 = lme(persoil_plot_t~species_diversity,syndata,random=~1|block,method="REML")
anova(model4)
summary(glht(model4,linfct = mcp(species_diversity="Tukey")))
Anova(model4,type="II")

# model evalution
#plot(model4)
#plot(model4,recov_plot_t~fitted(.))
#qqnorm(model4,~resid(.)|block)
```

### Results of the showcase analysis

System 15N retention (overall plot recovery) was positively affected by species
richness at the level of P = 0.0576 (Chisq: 5.7; Fig. Xa). The analysis of the
different system compartments (leaves, fine roots and soil) revealed that fine
root recovery was lower than leaf recovery, and biomass recovery (leaves and
fine roots) was lower than soil recovery.  Whereas the relative leaf and root
recovery were significantly higher in species mixtures compared with
monoculture (Figs. Xb and Xc), the relative soil recovery was significantly
reduced (Fig. Xd).

Our results demonstrate that species richness of mixtures increases system N
retention in young subtropical tree plantations. Although relative soil
recovery was highest compared with relative leaf and root recovery, soil
recovery decreased with species richness (Fig. X). Thus, the observed positive
relationship between species richness and system N retention is caused by an
increase in relative N recovery of sapling biomass (fine roots and leaves) with
higher species numbers in mixtures. Our findings suggest positive species
diversity effects for an important ecosystem service, which is highly relevant
for afforestation programmes as currently applied in China on a large scale.
The positive relationship between species richness and system N retention
suggests that mixed plantings even at the sapling stage of a restricted species
pool may considerably increase N retention in subtropical forest systems even
after a few years. This in turn has the potential to significantly reduce N
losses and thus N accumulation in the leachate or groundwater (Lang et al.
2013).

```{r anne_final_plot, echo = F}
attach(syndata)
par(mfrow=c(2,2),bty="l")
boxplot(recov_plot~species_diversity,las=1,ylab="N retention (at.%)",col="gray",
        xlab="Species richness")
text(3.4,31,"(A)",font=2)
boxplot(perleaf_plot~species_diversity,las=1,ylab=" Relative leaf recovery (%) ",
        col=c("dark gray","white","light gray"),xlab="Species richness")
text(1,25,"b",font=2);text(2,25,"a",font=2);text(3,25,"ab",font=2);text(3.4,27,"(B)",font=2)
boxplot(perroot_plot~species_diversity,las=1,ylab="Relative root recovery (%)",
        col=c("dark gray","white","white"),xlab="Species richness")
text(1,8.3,"b",font=2);text(2,8.3,"a",font=2);text(3,8.3,"a",font=2);text(3.4,8.3,"(C)",font=2)
boxplot(persoil_plot~species_diversity,las=1,ylab="Relative soil recovery (%)",
        col=c("white","light gray","dark gray"),xlab="Species richness")
text(1,70,"a",font=2);text(2,70,"ab",font=2);text(3,70,"b",font=2);text(3.4,98,"(D)",font=2)
```

* caption:  Nitrogen (N) retention affected by species richness. N retention summed as
            the recovery of soil, roots and leaves (a), relative leaf recovery (b), relative root
            recovery (c) and relative soil recovery (d). Significant differences as revealed by post
            hoc Tukey’s test (P < 0.05) are indicated by different letters.

Finally we need to decide on either to upload a full dataset which we could do
using the dataset upload function `bef.portal.upload.dataset()` or only to
upload the script and maybe a figure to highlight the road to the results from
the source datasets used. This can be done by attaching to the proposal with
`bef.portal.attach.to_proposal()`. For the example shown, it is the best way to
go with an attachment to the proposal as uploading the merged dataset would
only mean duplication of data in the database of the `BEFdata` platform.

So we upload the script and the four pane figure with its caption to add them
to the proposal. The figure is prepared as Portable Network Graphics (PNG)
using the R internal PNG device. It is written to a temporary folder and then
uploaded. The script in this case is an R markdown file that we attach to the
proposal (see box below).

```{r upload_script, include = T, eval = T, cache = T}
# attach the script using a path  
bef.portal.attach.to_proposal(id = 90, attachment = "./rbefdata.Rmd", description = "The R script that has been used to derive the results in the published paper")
```

```{r prepare_plot_png, echo = F, cache = F}
# prepare the plot figure as png
png(filename = file.path(tempdir(), "results_plot_proposal_90.png"))
  par(mfrow=c(2,2),bty="l")
  boxplot(recov_plot~species_diversity,las=1,ylab="N retention (at.%)",col="gray",
          xlab="Species richness")
  text(3.4,31,"(A)",font=2)
  boxplot(perleaf_plot~species_diversity,las=1,ylab=" Relative leaf recovery (%) ",
          col=c("dark gray","white","light gray"),xlab="Species richness")
  text(1,25,"b",font=2);text(2,25,"a",font=2);text(3,25,"ab",font=2);text(3.4,27,"(B)",font=2)
  boxplot(perroot_plot~species_diversity,las=1,ylab="Relative root recovery (%)",
          col=c("dark gray","white","white"),xlab="Species richness")
  text(1,8.3,"b",font=2);text(2,8.3,"a",font=2);text(3,8.3,"a",font=2);text(3.4,8.3,"(C)",font=2)
  boxplot(persoil_plot~species_diversity,las=1,ylab="Relative soil recovery (%)",
          col=c("white","light gray","dark gray"),xlab="Species richness")
  text(1,70,"a",font=2);text(2,70,"ab",font=2);text(3,70,"b",font=2);text(3.4,98,"(D)",font=2)
dev.off()
```

```{r caption_and_upload_figure, cache = T}
# caption of the figure
caption = "Nitrogen (N) retention affected by species richness. N retention summed as
           the recovery of soil, roots and leaves (a), relative leaf recovery (b), relative root
           recovery (c) and relative soil recovery (d). Significant differences as revealed by post
           hoc Tukey’s test (P < 0.05) are indicated by different letters."  

# upload the figure
bef.portal.attach.to_proposal(id = 90, attachment = file.path(tempdir(), "results_plot_proposal_90.png"), description = caption)  
``` 

![showcase_proposal_attachment](./figure/static/showcase_proposal_attachments.png)

* caption: The paper proposal in its final approved state with attachments. The
           attachments are the final results figure and the R script that has been 
           used to derive the results in the published paper.

## Discussion

* pros and cons of sending data to a local script or sending a script to a central database,
  - for researchers, it "feels" better, if the data are on the local machines
  - more freedom for researchers to use their tools of choice and to mix data with data from
    other sources
  - pros for sending a script to the data are efficiency, less network traffic
  - re-usability of scripts is easier, if they are sent to a central cluster, since everybody can
    do that.

* General discussion
  - high need to effective use/reuse data
  - relevant data needs to be simply detectable

* `BEFdata` and `rbefdata`
  - in combination provide a solution to
    + data storage
    + describing data with metadata
    + collaboration and data sharing
    + simply pull data into analysis software and push data back
    + data provenance by attaching R scripts to uploads
  - will provide solution with next versions
    + easier finding relevant data
    + smart merges (including unit conversions)

There is a growing demand to effectively use and reuse available data which
puts much pressure on the development of software solutions that help
researchers not only to find but also to integrate heterogeneous small data
into a wider context (cite xxx). The software combination `rbefdata`/`BEFdata`
provides solutions to different parts of the data life cycle. On the `BEFdata`
side it covers data storage, metadata support and social components that foster
sharing data online in collaborations (cite Karin).  On `rbefdata` side the
combination offers easy access to data and metadata which helps to understand
and use data stored on a `BEFdata` platform.  Additionally it offers methods to
simply push back datasets and attachments like plots and scripts to the
`BEFdata` portal which is a stepping stone for reproducibility and data
provenance. 

The tag based exploration of datasets helps to find datasets relevant for a
certain analysis. The `tematres` vocabulary server integration further supports
this as it allows to retrieve term definitions as well as semantical relations
to broaden or narrow down search terms. This feature is part of the upcoming
integration of an ontology based on the vocabulary and knowledge of
Biodiversity Ecosystem Functioning Experiment

The `rbefdata` package makes the access to the data and metadata simple. The
availability available via the R statistics environment with allows for fast
analysis. The upload mechanisms of the package help to keep the Online platform
up to date and gives other researchers the possibility to reproduce the results
by downloading scripts attached to proposals. The uploaded script is not only a
stepping stone to reproducible research but also helps to track down data
provenance.

Especially domains with a high degree of interdisciplinary interactions and
heterogeneity in methods and data like ecology are facing problems in dealing
with some highly valuable concepts of data management like ontologies (e.g
                                                                       michener
                                                                       et al
                                                                       2012). 







We recently stared to develop an ontology using a `tematres` server containing
knowledge extracted from portals that deal with data management for ecological
research. The `tematres` server offers an API so all the contained terms can be
accessed by the upcoming version of `rbefdata`

The formalization developed will be based on the knowledge used in biodiversity
research. Thus we will here discuss the software combination `BEFdata` and
`rbefdata` in the light of the upcoming features and in general context state
of the art data management today. In one of the next versions to be rolled out
the `BEFdata` portal will get a semantical annotation feature.  This will give
administrators the ability to tag each column of datasets with a general term
that best describes the content. So the field will contain potential top terms
of the ontology. The tagging will be reflected in the API and can thus be
simply queried to use the information within the R package.  Using the
knowledge about the content of a column in the R package will enable us to do
support smart merges that work.

`tematres` ([homepage](http://www.vocabularyserver.com/))into `BEFdata` and the
`rbefdata` package so they play well together semantically.
    
While well described data can help a lot in understanding datasets and on
deciding upon the relevance and applicability in a certain analysis there is
still lots of manual intervention necessary after that to prepare the data for
analysis (cite Karin and me? or xxx). It may needs to be cleaned, imputed,
reshaped and merged which usually takes up to 70% of an analysis workflow,
before smart models can be applied to the data to find interesting patters
(cite the workflow paper of Karin and me). This preparatory steps not only are
time and labour intensive but also potentially error prone, especially as the
complexity of the analyses increases.

Those potentially can be used to improve or automate some of the most common
tasks in analyses starting from finding relevant data to cleaning and merging
as well as they can facilitate the exchange of data.  

Ontologies, as formal representations of knowledge, potentially offer a
sophisticated tool to deal with that step of data preparation (cite supporting
ecology as data intensive science). While they are already used in some
research domains like genetics (cite xxx, eg. http://www.geneontology.org/),
other domains face more problems using it (cite xxx, morpho team announced
semantic tagging but the plug-in did not appear anywhere). The application of
ontologies in ecology is discussed controversially (cite xxx) and it is argued
that they can be a huge benefit, but it is hard to set up a sophisticated
ontology covering all necessary terms and relation of a highly complex research
domains (cite xxx).




While data storage and description is almost the same for all kind of data the
effective interlinking of data via an ontology requires the development of a
common terminology all contributing scientist of a research domain accept and
not only use but help do develop and discuss it.  Thus collaborative ontology
engineering approaches like `ontoverse` (Zoulfa El Jerroudi et al. 2008) or
`tematres` are highly valuable as the not only help to set up ontologies but
also to develop and maintain them over time even if the researchers change that
contribute to it.


## Acknowledgements

Thanks to all the data owners of the proposal for providing access to the
datasets. ...

## Literature

This will be done externally as markdown has no good way to deal with
references and stuff. We can collect here a list of links to references. I will
then collect them with e.g zotero to export a format we can hand them in for
publication.

## Appendix

### Figures

### saveaway 

We discuss `rbefdata` and `BEFdata` in in the light of current and future
challenges for data management give an outlook onto upcoming features that
could help to solve them. 


The data here is mainly provided by small scale studies spread all over the
world (e.g heidorn2009 shedding light on the dark) but also through bigger long
term projects like LTER (cite xxx), BEF-China (cite xxx), governmental projects
and local initiatives (cite xxx) and private persons. This in fact results in a
wild growing, complex and heterogeneous data landscape that an ontology would
need to capture to be usable.

